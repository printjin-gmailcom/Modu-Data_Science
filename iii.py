# -*- coding: utf-8 -*-
"""iii.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F_yJWX1ljaGMkxdvo3HBHTmt2Xgdboz5
"""

from bs4 import BeautifulSoup
from urllib.request import urlopen

soup1 = BeautifulSoup("<HTML><HEAD><header></HEAD><body></HTML>")

soup2 = BeautifulSoup(open("myDoc.html"))

soup3 = BeautifulSoup(urlopen("http://www.networksciencelab.com/"))

htmlString = ''' <HTML>
  <HEAD><TITLE>My document</TITLE></HEAD>
  <BODY>Main text.</BODY></HTML>
'''
soup = BeautifulSoup(htmlString)
soup.get_text()

with urlopen("http://www.networksciencelab.com/") as doc:
  soup = BeautifulSoup(doc)

links = [(link.string, link["href"])
    for link in soup.find_all("a")
    if link.has_attr("href")]
links



# https://data.gov/

import csv
with open("Demographic_Statistics_By_Zip_Code.csv", newline='') as infile:
  data = list(csv.reader(infile))

countParticipantsIndex = data[0].index("COUNT PARTICIPANTS")
countParticipantsIndex

import statistics
countParticipants = [int(row[countParticipantsIndex]) for row in data[1:]]
print(statistics.mean(countParticipants), statistics.stdev(countParticipants))



with open("data.json", "w") as out_json:
  json.dump(object, out_json, indent=None, sort_keys=False)

with open("data.json", "w") as in_json:object1 = json.load(in_json)

json_string = json.dumps(object1)

object2 = json.loads(json_string)



import nltk #nltk.corpus
nltk.download()

wn = nltk.corpus.wordnet # 코퍼스 리더(reader)
wn.synsets("cat")

wn.synset("cat.n.01").hypernyms() #상위어
wn.synset("cat.n.01").hyponyms() #하위어

x = wn.synset("cat.n.01")
y = wn.synset("lynx.n.01")
x.path_similarity(y)

[simxy.definition() for simxy in max(
  (x.path_similarity(y), x, y)
  for x in wn.synsets('cat')
  for y in wn.synsets('dog')
  if x.path_similarity(y)
)[1:]]

# myCorpus.fileids(), myCorpus.raw(), myCorpus.sents(), myCorpus.words()

from nltk.tokenize import WordPunctTokenizer
word_punct = WordPunctTokenizer()
text = "}Help! :))) :[ ..... :D{"
word_punct.tokenize(text)

nltk.word_tokenize(text)

from nltk.stem import PorterStemmer
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem import WordNetLemmatizer

pstemmer = nltk.PorterStemmer()
pstemmer.stem("wonderful")

lstemmer = nltk.LancasterStemmer()
lstemmer.stem("wonderful")

lemmatizer = nltk.WordNetLemmatizer()
lemmatizer.lemmatize("wonderful")

nltk.pos_tag(["beautiful", "world"])

from bs4 import BeautifulSoup
from collections import Counter
from nltk.corpus import stopwords
from nltk import LancasterStemmer

ls = nltk.LancasterStemmer()

with open("index.html") as infile:
  soup = BeautifulSoup(infile)

words = nltk.word_tokenize(soup.text)

words = [w.lower() for w in words]

words = [ls.stem(w) for w in text if w not in stopwords.words("english") and w.isalnum()]

freqs = Counter(words)
print(freqs.most_common(10))

